{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q --force-reinstall airavata-python-sdk[notebook]\n",
    "import airavata_jupyter_magic\n",
    "\n",
    "%authenticate\n",
    "%request_runtime hpc_cpu --file=cybershuttle.yml --walltime=60 --use=NeuroData25VC1:cloud,expanse:shared,anvil:shared\n",
    "%switch_runtime hpc_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring `temporaldata`\n",
    "\n",
    "Notebook author: Mehdi Azabou | Contributions: Vinam Arora, Shivashriganesh Mahato, Eva Dyer\n",
    "\n",
    "***\n",
    "\n",
    "In this notebook, we will go through an example for preparing a dataset using\n",
    "data objects from **temporaldata**, which we will learn to manipulate. Then, we will learn how to use samplers from **torch_brain** as part of a deep learning workflow.\n",
    "\n",
    "The notebook is organized around three main concepts:\n",
    "- **Part 1: Data**\n",
    "- **Part 2: Dataset and slicing**\n",
    "- **Part 3: Samplers**\n",
    "\n",
    "Documentation:\n",
    "- [**temporaldata**](https://temporaldata.readthedocs.io)\n",
    "- [**torch_brain**](https://torch-brain.readthedocs.io)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Part 1: Data\n",
    "\n",
    "In part 1, we will learn about temporal data objects, and go through the process of preparing a dataset.\n",
    "\n",
    "***\n",
    "\n",
    "## Table of contents:\n",
    "* [1.1 Overview of **temporaldata**](#a)\n",
    "* [1.2 Downloading a recording from DANDI](#c)\n",
    "* [1.3 Data Objects in temporaldata](#d)\n",
    "* [1.4 Bringing it all together: Data](#e)\n",
    "* [1.5 Saving data to disk](#f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 1.1 Overview of **temporaldata** <a class=\"anchor\" id=\"a\"></a>\n",
    "\n",
    "<img src=\"https://temporaldata.readthedocs.io/en/latest/_static/temporaldata_logo.png\" width=\"150\" height=\"150\" alt=\"temporaldata Logo\">\n",
    "\n",
    "[Documentation](https://temporaldata.readthedocs.io)\n",
    "\n",
    "**temporaldata** is a Python package designed for deep learning workflows. It provides data abstractions for multi-modal, multi-resolution temporal data that are both intuitive and efficient. You can install it using pip.\n",
    "\n",
    "\n",
    "#### Motivation:\n",
    "- Temporal data in the real world is messy, and usually involves multiple multi-modal streams of data, recorded across different sensors. An example of this is neural recordings, where brain activity and behavior are typically recorded simultaneously. Such datasets often consist of a combination of **regular** and **irregular** time series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! dandi download dandi://dandi/000688@draft/sub-T/sub-T_ses-CO-20130819_behavior+ecephys.nwb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a sessions from [1]. The monkey is performing a center-out task, where it makes a reach to one of eight fixed targets after receiving a go cue, and then returning to the center.\n",
    "\n",
    "<img src=\"https://ik.imagekit.io/7tkfmw7hc/tbrain_notebooks/mp_image.png?updatedAt=1744132572759\" width=\"300\" alt=\"temporaldata Logo\">\n",
    "\n",
    "---\n",
    "\n",
    "[1] Perich, Matthew G.; Miller, Lee E.; Azabou, Mehdi; Dyer, Eva L. (2025) Long-term recordings of motor and premotor cortical spiking activity during reaching in monkeys (Version 0.250122.1735) [Data set]. DANDI Archive. https://doi.org/10.48324/dandi.000688/0.250122.1735"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://poyo-brain.github.io/assets/bk-tap.png\" width=\"50\" alt=\"This cell is interactive\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define plotting utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.layouts import column\n",
    "from bokeh.models import ColumnDataSource, RangeTool\n",
    "import numpy as np\n",
    "\n",
    "from bokeh.models import Button\n",
    "from bokeh.models.callbacks import CustomJS\n",
    "from bokeh.layouts import row, column\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "def plot_spikes(spikes, add_range_tool=False, x_range=None, width=800, height=400):\n",
    "    \"\"\"\n",
    "    Plots an IrregularTimeSeries object defined by spikes.timestamps and spikes.unit_index.\n",
    "\n",
    "    Parameters:\n",
    "    spikes: An object containing 'timestamps' and 'unit_index' attributes.\n",
    "    \"\"\"\n",
    "    if x_range is None:\n",
    "        if add_range_tool:\n",
    "            x_range = (spikes.timestamps[0] * 1e3, spikes.timestamps[0] * 1e3 + 20_000)\n",
    "        else:\n",
    "            x_range = (spikes.timestamps[0] * 1e3, spikes.timestamps[-1] * 1e3)\n",
    "\n",
    "    # Create a figure\n",
    "    p = figure(x_axis_label='Time', y_axis_label='Unit Index', width=width, height=height, x_axis_type=\"datetime\", x_range=x_range, title=\"Spikes\")\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    x_values = spikes.timestamps * 1e3\n",
    "    y_values = spikes.unit_index\n",
    "\n",
    "    # Create a ColumnDataSource\n",
    "    source = ColumnDataSource(data=dict(x=x_values, y=y_values))\n",
    "\n",
    "    # Add scatter points to the plot\n",
    "    p.scatter('x', 'y', source=source, size=5, color=\"navy\", alpha=0.5, marker=\"dash\", angle=np.pi/2)\n",
    "\n",
    "    if add_range_tool:\n",
    "        select = figure(height=height//5, width=width, tools=\"\",\n",
    "                        toolbar_location=None, background_fill_color=\"#efefef\", x_axis_type=\"datetime\",\n",
    "                        title=\"Average Population Activity\")\n",
    "        select.xaxis.visible = False\n",
    "        # select.yaxis.visible = False\n",
    "\n",
    "        range_tool = RangeTool(x_range=p.x_range)\n",
    "        range_tool.overlay.fill_color = \"navy\"\n",
    "        range_tool.overlay.fill_alpha = 0.2\n",
    "\n",
    "        spike_times_int = spikes.timestamps.astype(int)\n",
    "        population_activity =np.bincount(spike_times_int-spike_times_int[0])\n",
    "        source = ColumnDataSource(data=dict(x=(np.arange(len(population_activity)) + spike_times_int[0])* 1e3, y=population_activity))\n",
    "\n",
    "        select.line('x', 'y', source=source)\n",
    "        select.ygrid.grid_line_color = None\n",
    "        select.add_tools(range_tool)\n",
    "        p = column(select, p)\n",
    "\n",
    "    return p\n",
    "\n",
    "def plot_time_series(data, field, index=None, x_range=None, add_range_tool=False, y_axis_label=None, width=800, height=200, include_control_panel=False, shade_area_list=[]):\n",
    "    # Create a figure\n",
    "    if x_range is None:\n",
    "        if add_range_tool:\n",
    "            x_range = (data.timestamps[0] * 1e3, data.timestamps[0] * 1e3 + 20_000)\n",
    "        else:\n",
    "            x_range = (data.timestamps[0] * 1e3, data.timestamps[-1] * 1e3)\n",
    "\n",
    "    if y_axis_label is None:\n",
    "        y_axis_label = field\n",
    "\n",
    "    p = figure(x_axis_label='Time', y_axis_label=y_axis_label, width=width, height=height, x_axis_type=\"datetime\", x_range=x_range)\n",
    "\n",
    "\n",
    "    domain_start = data.domain.start * 1e3\n",
    "    domain_end = data.domain.end * 1e3\n",
    "    # Prepare data for plotting\n",
    "    x_values = data.timestamps * 1e3\n",
    "    y_values = getattr(data, field)\n",
    "\n",
    "    x_values = np.concatenate([x_values, domain_start, domain_end])\n",
    "    y_values = np.concatenate([y_values, np.nan * np.ones((len(data.domain), * y_values.shape[1:])), np.nan * np.ones((len(data.domain), *y_values.shape[1:]))])\n",
    "\n",
    "    # sort x_values and reorder y_values\n",
    "    sort_indices = np.argsort(x_values)\n",
    "    x_values = x_values[sort_indices]\n",
    "    y_values = y_values[sort_indices]\n",
    "\n",
    "    if y_values.ndim == 1:\n",
    "        # Add a line to the plot\n",
    "        source = ColumnDataSource(data=dict(x=x_values, y=y_values))\n",
    "    elif y_values.ndim == 2:\n",
    "        assert index is not None, \"Index must be provided for 2D data\"\n",
    "        source = ColumnDataSource(data=dict(x=x_values, y=y_values[:, index]))\n",
    "    else:\n",
    "        raise ValueError(f\"Field {field} has {y_values.ndim} dimensions, expected 1 or 2\")\n",
    "\n",
    "    p.line(x='x', y='y', source=source, line_width=2, color=\"green\")\n",
    "    x_range = p.x_range\n",
    "\n",
    "    colors = ['red', 'blue', 'green', 'yellow', 'purple', 'orange', 'brown', 'pink', 'gray', 'black']\n",
    "    for i, shade_area in enumerate(shade_area_list):\n",
    "        start = shade_area.start * 1e3\n",
    "        end = shade_area.end * 1e3\n",
    "        rect_x = (start + end)/2\n",
    "        rect_width = end - start\n",
    "        rect_y = np.ones_like(start) * (y_values.min() + y_values.max())/2\n",
    "        rect_height = np.ones_like(start) * (y_values.max() - y_values.min())\n",
    "        p.rect(x=rect_x, y=rect_y, width=rect_width, height=rect_height, fill_color=colors[i], fill_alpha=0.4)\n",
    "        # legend_items.append((f\"Shade Area {i+1}\", [p.renderers[-1]]))  # Add the rectangle to the legend items\n",
    "\n",
    "    # p.legend.items = legend_items\n",
    "    # p.legend.location = \"top_left\"  # Set the legend location\n",
    "    # p.legend.click_policy = \"hide\"  # Allow clicking to hide the legend items\n",
    "\n",
    "    if add_range_tool:\n",
    "        select = figure(height=height//5, width=width, y_range=p.y_range, tools=\"\",\n",
    "                        toolbar_location=None, background_fill_color=\"#efefef\", x_axis_type=\"datetime\",)\n",
    "        select.xaxis.visible = False\n",
    "        select.yaxis.visible = False\n",
    "\n",
    "        range_tool = RangeTool(x_range=p.x_range)\n",
    "        range_tool.overlay.fill_color = \"navy\"\n",
    "        range_tool.overlay.fill_alpha = 0.2\n",
    "\n",
    "        select.line('x', 'y', source=source)\n",
    "        select.ygrid.grid_line_color = None\n",
    "        select.add_tools(range_tool)\n",
    "        p = column(select, p)\n",
    "\n",
    "        if include_control_panel:\n",
    "            # Add a button to control the range tool\n",
    "            play_button = Button(label=\"Play\", button_type=\"success\")\n",
    "            pause_button = Button(label=\"Pause\", button_type=\"warning\")\n",
    "            speed_buttons = [Button(label=f\"{2**i}x\", button_type=\"primary\") for i in range(5)]\n",
    "\n",
    "            # Create a shared ColumnDataSource to store the interval ID\n",
    "            shared_data = ColumnDataSource(data=dict(interval_id=[None], step_size=[1000]))\n",
    "\n",
    "            # Update the CustomJS to use the shared data source\n",
    "            play_button.js_on_click(CustomJS(args=dict(range_tool=range_tool, x_values=x_values, shared_data=shared_data), code=\"\"\"\n",
    "                // Clear previous interval if it exists\n",
    "                if (shared_data.data.interval_id[0]) {\n",
    "                    clearInterval(shared_data.data.interval_id[0]);\n",
    "                }\n",
    "\n",
    "                // Create new interval and store it in the shared data\n",
    "                let new_interval = setInterval(() => {\n",
    "                    if (range_tool.x_range.end < x_values[x_values.length - 1]) {\n",
    "                        range_tool.x_range.start += shared_data.data.step_size[0] / 10;\n",
    "                        range_tool.x_range.end += shared_data.data.step_size[0] / 10;\n",
    "                    } else {\n",
    "                        clearInterval(shared_data.data.interval_id[0]);\n",
    "                        shared_data.data.interval_id[0] = null;\n",
    "                        shared_data.change.emit();\n",
    "                    }\n",
    "                }, 100);\n",
    "\n",
    "                shared_data.data.interval_id[0] = new_interval;\n",
    "                shared_data.change.emit();\n",
    "            \"\"\"));\n",
    "\n",
    "            pause_button.js_on_click(CustomJS(args=dict(shared_data=shared_data), code=\"\"\"\n",
    "                if (shared_data.data.interval_id[0]) {\n",
    "                    clearInterval(shared_data.data.interval_id[0]);\n",
    "                    shared_data.data.interval_id[0] = null;\n",
    "                    shared_data.change.emit();\n",
    "                }\n",
    "            \"\"\"));\n",
    "\n",
    "            for button in speed_buttons:\n",
    "                button.js_on_click(CustomJS(args=dict(button=button, shared_data=shared_data), code=\"\"\"\n",
    "                    shared_data.data.step_size[0] = 1000 * parseInt(button.label.replace('x', ''));\n",
    "                    shared_data.change.emit();\n",
    "                \"\"\"));\n",
    "\n",
    "            # Add the buttons to the layout\n",
    "            button_layout = row(play_button, pause_button, *speed_buttons)\n",
    "            return p, x_range, button_layout\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "def plot_intervals(*interval, x_range=None, title=None, width=800, height=200):\n",
    "    colors = [\"red\", \"blue\", \"green\", \"yellow\", \"orange\", \"purple\", \"pink\", \"brown\", \"black\"]\n",
    "    # Create a figure\n",
    "    if x_range is None:\n",
    "      p = figure(title=title, x_axis_label='Time', y_axis_label='Intervals', y_range=(-len(interval), 1), width=width, height=height, x_axis_type=\"datetime\",)\n",
    "    else:\n",
    "      p = figure(title=title, x_axis_label='Time', x_range=x_range, y_axis_label='Intervals', y_range=(-len(interval), 1), width=width, height=height, x_axis_type=\"datetime\",)\n",
    "\n",
    "    p.yaxis.visible = False\n",
    "\n",
    "    for i in range(len(interval)):\n",
    "        # Prepare data for plotting\n",
    "        centers = (interval[i].start + interval[i].end) / 2.  * 1e3\n",
    "        durations = (interval[i].end - interval[i].start)  * 1e3\n",
    "        y_values = np.zeros_like(centers) - i  # y-values for the intervals\n",
    "\n",
    "        # Create a ColumnDataSource\n",
    "        source = ColumnDataSource(data=dict(x=centers, width=durations, y=y_values))\n",
    "\n",
    "        # Add rectangles to the plot\n",
    "        p.rect(x='x', y='y', width='width', height=0.8, source=source, fill_color=colors[i % 10], line_color=\"black\", alpha=0.5)\n",
    "\n",
    "    return p\n",
    "\n",
    "def make_plot(data, add_play_controls=False):\n",
    "    include_control_panel = add_play_controls\n",
    "    if include_control_panel:\n",
    "      p_cursor_pos_x, x_range, button_layout = plot_time_series(data.cursor, 'vel', y_axis_label='cursor position x', index=0, add_range_tool=include_control_panel, include_control_panel=include_control_panel, shade_area_list=[data.domain])\n",
    "    else:\n",
    "      p_cursor_pos_x = plot_time_series(data.cursor, 'vel', y_axis_label='cursor position x', index=0, add_range_tool=include_control_panel, include_control_panel=include_control_panel, shade_area_list=[data.domain])\n",
    "      x_range=p_cursor_pos_x.x_range\n",
    "\n",
    "    p_cursor_pos_y = plot_time_series(data.cursor, 'vel', y_axis_label='cursor position y', index=1, add_range_tool=include_control_panel, x_range=x_range, shade_area_list=[data.reach_intervals])\n",
    "    p_cursor_vel_x = plot_time_series(data.cursor, 'vel', y_axis_label='cursor velocity x', index=0, add_range_tool=include_control_panel, x_range=x_range, shade_area_list=[data.reach_intervals])\n",
    "    p_cursor_vel_y = plot_time_series(data.cursor, 'vel', y_axis_label='cursor velocity y', index=1, add_range_tool=include_control_panel, x_range=x_range, shade_area_list=[data.reach_intervals])\n",
    "    p_spikes = plot_spikes(data.spikes, add_range_tool=include_control_panel, x_range=x_range, height=480)\n",
    "    p_reach_intervals = plot_intervals(data.reach_intervals, x_range=x_range, height=120, title=\"Reach intervals\")\n",
    "\n",
    "    if include_control_panel:\n",
    "      return column(button_layout, row(p_spikes, column(p_reach_intervals, p_cursor_vel_x, p_cursor_vel_y)))\n",
    "    else:\n",
    "      return row(p_spikes, column(p_reach_intervals, p_cursor_vel_x, p_cursor_vel_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we can visualize the data. We can see the simultanously recorded spiking activity and the corresponding cursor velocity. Press \"Play\" to start playing the recording back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Visualizing the data\n",
    "\n",
    "def plot_data():\n",
    "  from pynwb import NWBHDF5IO\n",
    "  import numpy as np\n",
    "  from temporaldata import ArrayDict, IrregularTimeSeries, RegularTimeSeries, Interval, Data\n",
    "\n",
    "  input_file = \"sub-T_ses-CO-20130819_behavior+ecephys.nwb\"\n",
    "  io = NWBHDF5IO(input_file, \"r\")\n",
    "  nwbfile = io.read()\n",
    "\n",
    "  spike_train_list = nwbfile.units.spike_times_index[:]\n",
    "\n",
    "  unit_ids = []\n",
    "  unit_brain_areas = []\n",
    "  for i in range(len(spike_train_list)):\n",
    "      unit_ids.append(f\"unit_{i}\")\n",
    "      unit_brain_areas.append(nwbfile.units.electrodes[i].location.item())\n",
    "  units = ArrayDict(id=np.array(unit_ids), brain_area=np.array(unit_brain_areas))\n",
    "\n",
    "  spike_timestamps = np.array([])\n",
    "  spike_unit_index = np.array([])\n",
    "\n",
    "  for i in range(len(spike_train_list)):\n",
    "      spike_train = spike_train_list[i]\n",
    "      spike_timestamps = np.concatenate([spike_timestamps, spike_train])\n",
    "      spike_unit_index = np.concatenate([spike_unit_index, np.full_like(spike_train, fill_value=i)])\n",
    "\n",
    "  spikes = IrregularTimeSeries(\n",
    "      timestamps=spike_timestamps,\n",
    "      unit_index=spike_unit_index,\n",
    "      domain=\"auto\",\n",
    "  )\n",
    "  spikes.sort()\n",
    "\n",
    "  timestamps = nwbfile.processing[\"behavior\"][\"Position\"][\"cursor_pos\"].timestamps[:]\n",
    "  cursor_pos = nwbfile.processing[\"behavior\"][\"Position\"][\"cursor_pos\"].data[:]\n",
    "  cursor_vel = nwbfile.processing[\"behavior\"][\"Velocity\"][\"cursor_vel\"].data[:]\n",
    "  cursor_acc = nwbfile.processing[\"behavior\"][\"Acceleration\"][\"cursor_acc\"].data[:]\n",
    "\n",
    "  sampling_rate = 100 # Hz\n",
    "  assert np.allclose(np.diff(timestamps), 1/sampling_rate)\n",
    "\n",
    "  cursor = RegularTimeSeries(\n",
    "      pos=cursor_pos,\n",
    "      vel=cursor_vel,\n",
    "      acc=cursor_acc,\n",
    "      sampling_rate=sampling_rate,\n",
    "      domain_start=timestamps[0],\n",
    "      domain=\"auto\",\n",
    "  )\n",
    "\n",
    "  trial_table = nwbfile.trials.to_dataframe().dropna()\n",
    "  reach_intervals = Interval(\n",
    "      start=trial_table.go_cue_time.values,\n",
    "      end=trial_table.stop_time.values,\n",
    "      result=trial_table.result.values,\n",
    "      target_id=trial_table.target_id.values,\n",
    "  )\n",
    "\n",
    "  data = Data(\n",
    "    spikes=spikes,\n",
    "    cursor=cursor,\n",
    "    reach_intervals=reach_intervals,\n",
    "    domain=\"auto\",\n",
    "  )\n",
    "\n",
    "  p = make_plot(data, add_play_controls=True)\n",
    "  show(p)\n",
    "\n",
    "plot_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was loaded using **temporaldata** objects. In this part of the tutorial, we will walk through how to prepare the data using various abstractions, that make data manipulation much more natural and efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Download the data from DANDI as NWB file*\n",
    "\n",
    "The data is stored in an NWB file. The Neurodata Without Borders (NWB) format is a standardized, open-source file format designed to store neurophysiology data, including electrophysiology, and behavioral information.\n",
    "We will use the `pynwb` library to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynwb import NWBHDF5IO\n",
    "\n",
    "input_file = \"sub-T_ses-CO-20130819_behavior+ecephys.nwb\"\n",
    "io = NWBHDF5IO(input_file, \"r\")\n",
    "nwbfile = io.read()\n",
    "\n",
    "# we can access the spike times and unit indices directly from the NWB file\n",
    "spike_train_list = nwbfile.units.spike_times_index[:]\n",
    "print(f\"Number of units found: {len(spike_train_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Data Objects in temporaldata\n",
    "\n",
    "We now introduce the main data objects in temporal data:\n",
    "- ArrayDict\n",
    "- IrregularTimeSeries\n",
    "- RegularTimeSeries\n",
    "- Interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Using **ArrayDict** to define the unit table <a class=\"anchor\" id=\"c\"></a>\n",
    "\n",
    "The `ArrayDict` is a simple container for numpy arrays that share the same first dimension.\n",
    "We can use this object to store the list of units including their identifiers (`id`) and brain areas (`brain_area`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from temporaldata import ArrayDict\n",
    "\n",
    "unit_ids = []\n",
    "unit_brain_areas = []\n",
    "\n",
    "# loop over the units\n",
    "for i in range(len(spike_train_list)):\n",
    "    unit_ids.append(f\"unit_{i}\")  # we will name the units as unit_0, unit_1, etc.\n",
    "    unit_brain_areas.append(nwbfile.units.electrodes[i].location.item())\n",
    "\n",
    "# create the ArrayDict object for the units\n",
    "units = ArrayDict(id=np.array(unit_ids), brain_area=np.array(unit_brain_areas))\n",
    "\n",
    "print(f\"Number of units: {len(units)}. Brain areas: {np.unique(units.brain_area)}\")\n",
    "print(f\"units={units}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Using **IrregularTimeSeries** for spikes\n",
    "\n",
    "Next, we will create the `spikes` object which will store the spiking neural data.\n",
    "\n",
    "- The `spike_train_list` we extracted from the NWB file is a list of numpy arrays, with each array containing the spike times for a single unit.\n",
    "In **temporaldata**, we combine all spikes into a large array. Each spike is defined by two values: the timestamp and the unit index.\n",
    "\n",
    "- Since the spiking data is irregular and sparse, we use the `IrregularTimeSeries` object to represent it. Instead of repeating the unit identifier (stored in `units.id`),\n",
    "we will use unit indices, where the unit index corresponds to a number between `0` and `num_units - 1` and maps to the unit id in `units.id`.\n",
    "\n",
    "- We will create the `spikes` object by looping over the spike trains and adding them to the `spike_timestamps` and `spike_unit_index` arrays. Then build our `IrregularTimeSeries` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from temporaldata import ArrayDict, IrregularTimeSeries\n",
    "\n",
    "# initialize the arrays\n",
    "spike_timestamps = np.array([], dtype=np.float64)\n",
    "spike_unit_index = np.array([], dtype=np.uint32)\n",
    "\n",
    "# loop over the spike trains and add them to the arrays\n",
    "for i in range(len(spike_train_list)):\n",
    "    spike_train = spike_train_list[i]\n",
    "    spike_timestamps = np.concatenate([spike_timestamps, spike_train])\n",
    "    spike_unit_index = np.concatenate([spike_unit_index, np.full_like(spike_train, fill_value=i, dtype=np.uint32)])\n",
    "\n",
    "# create the IrregularTimeSeries object\n",
    "spikes = IrregularTimeSeries(\n",
    "    timestamps=spike_timestamps,\n",
    "    unit_index=spike_unit_index,\n",
    "    domain=\"auto\",\n",
    ")\n",
    "# sort the spikes by timestamps\n",
    "spikes.sort()\n",
    "\n",
    "print(f\"Number of spikes: {len(spikes)}\")\n",
    "print(f\"spikes={spikes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spikes.timestamps[:10])\n",
    "print(spikes.unit_index[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://poyo-brain.github.io/assets/bk-tap.png\" width=\"50\" alt=\"This cell is interactive\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_spikes(spikes, width=1200)\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Using RegularTimeSeries for cursor data\n",
    "\n",
    "But next, we will create a `RegularTimeSeries` object to store the cursor position, velocity, and acceleration.\n",
    "A `RegularTimeSeries` represents uniformly sampled time series data. Unlike the `IrregularTimeSeries`, we do not need\n",
    " to provide timestamps as they are inferred from the sampling rate and the first timestamp in the recording.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from temporaldata import RegularTimeSeries\n",
    "\n",
    "timestamps = nwbfile.processing[\"behavior\"][\"Position\"][\"cursor_pos\"].timestamps[:]\n",
    "cursor_pos = nwbfile.processing[\"behavior\"][\"Position\"][\"cursor_pos\"].data[:]\n",
    "cursor_vel = nwbfile.processing[\"behavior\"][\"Velocity\"][\"cursor_vel\"].data[:]\n",
    "cursor_acc = nwbfile.processing[\"behavior\"][\"Acceleration\"][\"cursor_acc\"].data[:]\n",
    "\n",
    "sampling_rate = 100 # Hz\n",
    "assert np.allclose(np.diff(timestamps), 1/sampling_rate)\n",
    "\n",
    "cursor = RegularTimeSeries(\n",
    "    pos=cursor_pos,\n",
    "    vel=cursor_vel,\n",
    "    acc=cursor_acc,\n",
    "    sampling_rate=sampling_rate,\n",
    "    domain_start=timestamps[0],\n",
    "    domain=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the cursor position.\n",
    "\n",
    "<img src=\"https://poyo-brain.github.io/assets/bk-tap.png\" width=\"50\" alt=\"This cell is interactive\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_time_series(cursor, field=\"pos\", index=0, width=1200)\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4 Using Interval for the trial table\n",
    "\n",
    "The last object we will learn about in this tutorial is the `Interval` object.\n",
    "\n",
    "An `Interval` is a set of time intervals each defined by a start time and an end time, and that can have additional fields describing the intervals.\n",
    "\n",
    "We can extract the reach intervals, which are the time intervals between the go cue time and the time the movement stops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from temporaldata import Interval\n",
    "\n",
    "trial_table = nwbfile.trials.to_dataframe().dropna()\n",
    "reach_intervals = Interval(\n",
    "    start=trial_table.go_cue_time.values,\n",
    "    end=trial_table.stop_time.values,\n",
    "    result=trial_table.result.values,\n",
    "    target_id=trial_table.target_id.values,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://poyo-brain.github.io/assets/bk-tap.png\" width=\"50\" alt=\"This cell is interactive\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_intervals(reach_intervals, width=1200)\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Bringing it all together: Data\n",
    "\n",
    "All streams of data can be stored in a `Data` object, which is a container for multiple **temporaldata** objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from temporaldata import Data\n",
    "\n",
    "data = Data(\n",
    "    spikes=spikes,\n",
    "    units=units,\n",
    "    cursor=cursor.to_irregular(),\n",
    "    reach_intervals=reach_intervals,\n",
    "    domain=\"auto\",\n",
    "    # add metadata\n",
    "    brainset=Data(id=\"perich_miller_population_2018\"),\n",
    "    session=Data(id=\"t_20130819_center_out_reaching\"),\n",
    ")\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(make_plot(data, add_play_controls=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Saving data to disk\n",
    "**temporaldata** allows you to save files in hdf5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "\n",
    "recording_id = \"perich_miller_population_2018/t_20130819_center_out_reaching\"\n",
    "os.makedirs(\"perich_miller_population_2018/\", exist_ok=True)\n",
    "\n",
    "# Save to file to h5\n",
    "with h5py.File(f\"{recording_id}.h5\", \"w\") as f:\n",
    "    data.to_hdf5(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Dataset and slicing\n",
    "\n",
    "***\n",
    "\n",
    "## Table of contents:\n",
    "* [2.1 Slicing an IrregularTimeSeries object](#aa)\n",
    "* [2.2 Slicing a Data object](#bb)\n",
    "* [2.3 torch_brain.data.Dataset](#cc)\n",
    "\n",
    "***\n",
    "\n",
    "Training models often requires repeatedly accessing small segments of data randomly scattered across large datasets—extracting just a few seconds from hours of recordings. **temporaldata** has the following features:\n",
    "- easily slice through multi-modal data using only start and end times.\n",
    "- efficiently load just the data you need when you need it.\n",
    "\n",
    "Let's start with the first feature: slicing. Every temporal object in **temporaldata** has a `slice` method that can be called with two arguments: `start` and `end`. It returns an object of the same class but restricted to the data points within the `[start , end]` range.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://ik.imagekit.io/7tkfmw7hc/tbrain_notebooks/slicing.png?updatedAt=1744132610041\" width=\"1000\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1 Slicing an IrregularTimeSeries object\n",
    "Let's take a look at what this looks like for `spikes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes_sliced = spikes.slice(start=0., end=10.)\n",
    "\n",
    "print(f\"Before slicing:\\nspikes={spikes}\")\n",
    "print(f\"After slicing:\\nspikes_sliced={spikes_sliced}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned object is another `IrregularTimeSeries` object, but only contains spike events between `start` and `end`.\n",
    "\n",
    "Let's plot the sliced object.\n",
    "\n",
    "<img src=\"https://poyo-brain.github.io/assets/bk-tap.png\" width=\"50\" alt=\"This cell is interactive\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_spikes(spikes_sliced, width=1200)\n",
    "fig.title = f\"Spikes between {spikes_sliced.domain.start[0]}s and {spikes_sliced.domain.end[0]}s\"\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 2.2 Slicing a Data object\n",
    "\n",
    "More generally, a recording is represented by a multi-modal `Data` object. When `slice` is called, all of its children are sliced accordingly, making the slicing and object manipulation seemless.\n",
    "\n",
    "<img src=\"https://ik.imagekit.io/7tkfmw7hc/tbrain_notebooks/full_data_obj.png?updatedAt=1744132750893\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from pynwb import NWBHDF5IO\n",
    "\n",
    "input_file = \"sub-T_ses-CO-20130819_behavior+ecephys.nwb\"\n",
    "io = NWBHDF5IO(input_file, \"r\")\n",
    "nwbfile = io.read()\n",
    "\n",
    "# we can access the spike times and unit indices directly from the NWB file\n",
    "spike_train_list = nwbfile.units.spike_times_index[:]\n",
    "print(f\"Number of units found: {len(spike_train_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from pynwb import NWBHDF5IO\n",
    "\n",
    "input_file = \"sub-T_ses-CO-20130819_behavior+ecephys.nwb\"\n",
    "io = NWBHDF5IO(input_file, \"r\")\n",
    "nwbfile = io.read()\n",
    "\n",
    "# we can access the spike times and unit indices directly from the NWB file\n",
    "spike_train_list = nwbfile.units.spike_times_index[:]\n",
    "print(f\"Number of units found: {len(spike_train_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sliced = data.slice(start=20., end=100.)\n",
    "\n",
    "print(f\"Before slicing:\\ndata={data}\\n\")\n",
    "print(f\"After slicing:\\ndata_sliced={data_sliced}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(make_plot(data_sliced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
